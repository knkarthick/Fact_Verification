{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f19eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from Data_Preprocess_Cleaning import spacy_cleaner\n",
    "\n",
    "threshold1 = 0.01\n",
    "threshold2 = 0.629\n",
    "\n",
    "model1_path = './models/evidence_identification_model/model.epoch00-loss1.22'\n",
    "model2_path = './models/evidence_classification_model/model.epoch02-loss0.93'\n",
    "model_checkpoint_distilbert = \"distilbert-base-uncased\"\n",
    "model_checkpoint_bert = \"bert-base-uncased\"\n",
    "\n",
    "model1 = TFAutoModelForSequenceClassification.from_pretrained(model1_path,num_labels = 2)\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_checkpoint_distilbert)\n",
    "model2 = TFAutoModelForSequenceClassification.from_pretrained(model2_path,num_labels = 2)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_checkpoint_bert)\n",
    "\n",
    "def predict_proba(rumor, timeline, max_length, debug = 0):\n",
    "    tokenizer_output = tokenizer1(rumor, timeline, truncation=True, max_length = max_length, return_tensors = 'tf', padding=True)\n",
    "    logits = model1(**tokenizer_output)[\"logits\"]\n",
    "    predicted_proba_class_id = tf.nn.softmax(logits, axis=1).numpy()[0][1]\n",
    "    return predicted_proba_class_id\n",
    "\n",
    "def inference_pipe(fname, split):\n",
    "    EVLIST = []\n",
    "    FLIST = []\n",
    "    with open(fname, 'rb') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            print(data['id'], data.keys())\n",
    "\n",
    "            rumor_id = data['id']\n",
    "            rumor = data['rumor']\n",
    "\n",
    "            username = [entry[0] for entry in data['timeline']]\n",
    "            timeline_id = [entry[1] for entry in data['timeline']]\n",
    "            timelines = [entry[2] for entry in data['timeline']]\n",
    "\n",
    "            predictions = [predict_proba(spacy_cleaner(rumor), spacy_cleaner(timeline), max_length=192) for timeline in timelines]\n",
    "\n",
    "            tdf = pd.DataFrame()\n",
    "            tdf['username'] = username\n",
    "            tdf['timeline_id'] = timeline_id\n",
    "            tdf['timelines'] = timelines\n",
    "            tdf['predictions'] = predictions\n",
    "            tdf['id'] = rumor_id\n",
    "            tdf['rumor'] = rumor\n",
    "\n",
    "            tdf = tdf.sort_values(by=['predictions'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "            if tdf.shape[0]==0:\n",
    "                predicted_evidence = \"\"\n",
    "            else:\n",
    "                if tdf.shape[0]>5:\n",
    "                    tdff = tdf.iloc[0:5]\n",
    "                else:\n",
    "                    tdff = tdf\n",
    "                predicted_evidence = tdff[['username', 'timeline_id', 'timelines']].values.tolist()\n",
    "\n",
    "            tdf = tdf[tdf['predictions']>threshold1]\n",
    "\n",
    "            if tdf.shape[0]==0:\n",
    "                predictions_final = \"NOT ENOUGH INFO\"\n",
    "\n",
    "            else:\n",
    "                if (tdf.shape[0]>5):\n",
    "                    tdf = tdf.iloc[0:5]\n",
    "\n",
    "                tdf['rank'] = tdf.index + 1\n",
    "                tdf['Dummy'] = 'A0'\n",
    "                tdf['Team'] = 'KNK'\n",
    "\n",
    "                EVLIST.extend(tdf[['id', 'Dummy', 'timeline_id', 'rank', 'predictions', 'Team']].values.tolist())\n",
    "\n",
    "                predicted_evidence = tdf[['username', 'timeline_id', 'timelines']].values.tolist()\n",
    "\n",
    "                evidence = \" \".join(tdf['timelines'].values.tolist())\n",
    "\n",
    "\n",
    "                predictions_final = \"SUPPORTS\" if predict_proba(spacy_cleaner(rumor), spacy_cleaner(evidence), max_length=512) >= threshold2 else \"REFUTES\"\n",
    "            if split=='test':\n",
    "                FLIST.append([rumor_id, predictions_final, rumor, predicted_evidence])\n",
    "            else:\n",
    "                FLIST.append([rumor_id, predictions_final, rumor, data['label'], predicted_evidence])\n",
    "        \n",
    "    Evidence_retrieval_output = pd.DataFrame(EVLIST, columns = ['id', 'Dummy', 'timeline_id', 'rank', 'predictions', 'Team'])\n",
    "    Evidence_retrieval_output.to_csv('./results/Evidence_retrieval_output_'+split+'.csv', index=False)\n",
    "\n",
    "    if split=='test':\n",
    "        Rumor_Verification_output = pd.DataFrame(FLIST, columns = ['id', 'predicted_label', 'claim', 'predicted_evidence'])\n",
    "    else:\n",
    "        Rumor_Verification_output = pd.DataFrame(FLIST, columns = ['id', 'predicted_label', 'claim', 'label', 'predicted_evidence'])\n",
    "    Rumor_Verification_output.to_csv('./results/Rumor_Verification_output_'+split+'.csv', index=False)\n",
    "\n",
    "    Evidence_retrieval_output.to_csv(\"./results/KGAT_zeroShot_evidence_English_\"+split+\".txt\", sep=\"\\t\", index=None, header=False)\n",
    "\n",
    "    dic_list = Rumor_Verification_output.to_dict(orient='records')\n",
    "\n",
    "    with open('./results/KGAT_zeroShot_verification_English_'+split+'.json', 'w', encoding='utf-8') as output_file:\n",
    "        for dic in dic_list:\n",
    "            json.dump(dic, output_file) \n",
    "            output_file.write(\"\\n\")\n",
    "    print(\"Processing Done!!!\")\n",
    "\n",
    "inference_pipe('./data/original_data/English_dev.json', 'dev')\n",
    "inference_pipe('./data/original_data/English_test.json', 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
